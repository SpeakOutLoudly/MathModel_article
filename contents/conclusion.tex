\chapter[\hspace{0pt}总结与未来展望]{{\heiti\zihao{3}\hspace{0pt}总结与未来展望}}\label{section 7}
\removelofgap
\removelotgap

本章内容共分为两节，\hyperref[section5: 总结]{第一节}对本文研究内容与方法进行总结；\hyperref[section5: 未来展望]{第二节}介绍本文所提方法的局限性并对未来研究方向进行展望。

\section[\hspace{-2pt}总结]{{\heiti\zihao{-3} \hspace{-8pt}总结}}\label{section5: 总结}

少样本分类致力于模拟人类的知识迁移能力，期望模型在具有大量标注数据的基类数据上训练之后，能够将所学知识迁移至新类别，实现用少量标注样本进行有效学习。目前，少样本分类问题已取得一系列研究成果，但仍存在一些问题与挑战：特征提取网络迁移能力不够强；样本数量极少情况下无法捕获类别代表性特征。针对这些问题，本文分别从多粒度样本关系建模与语义-视觉多空间关系建模两个角度出发，对少样本分类中的多元关系进行了深入挖掘与研究。

（1）本文首先从多粒度样本关系建模的角度出发，开展了基于多粒度样本关系建模的少样本分类研究。针对少样本分类模型特征提取能力不足的问题，提出了一种基于多粒度样本关系对比学习的少样本特征学习算法：多粒度样本关系对比学习（Multi-Grained Sample Relation Contrastive Learning，简称MGSRCL）模型，旨在通过对不同粒度的样本关系进行建模以提升模型的特征提取能力。MGSRCL使用变换一致性学习来约束同一样本不同变换版本之间的样本内关系，通过使其预测概率分布相同令它们在语义内容上保持一致；使用类对比学习来约束同类样本的类内关系和不同类样本的类间关系，通过对其特征进行建模使同类样本语义内容相似、不同类样本语义内容不同。通过对多种粒度的样本关系细致地建模，MGSRCL提升了模型的特征提取能力，达到了优异的少样本分类结果。在miniImageNet、tieredImageNet、CIFAR-FS和CUB四个少样本基准数据集上的大量实验证明了MGSRCL的有效性。另外，通过将MGSRCL模型作为预训练模型与其他方法结合，证明了所获得特征提取网络的可迁移性。

（2）上述提出的MGSRCL方法虽然达到了优异的结果，但仍存在没有利用样本语义信息的问题。因此，以MGSRCL方法为基础，本文进一步进行了基于语义-视觉多空间关系建模的少样本分类研究。针对少量样本的视觉特征无法捕获类别代表性特征的问题，提出了一种基于语义-视觉多空间关系建模的少样本特征适配算法：语义-视觉多空间映射适配（Semantic-Visual Multi-Space Mapping Adapter，简称SVMSMA）模型，旨在引入语义信息对视觉信息进行补充，丰富样本特征的信息来源以提升其多样性与代表性。SVMSMA使用语义-视觉多空间映射网络将语义特征映射到视觉空间，并通过跨模态分类模块对单/多模态映射特征执行分类任务使其与视觉特征建立联系，以及跨模态特征对齐模块将映射特征与视觉特征原型进行对齐以获得更接近类别原型的特征。通过对语义-视觉多空间关系进行建模，SVMSMA丰富了样本特征的信息来源，提升了模型的泛化能力。在四个基准数据集上的实验证明了SVMSMA方法能够有效利用语义信息，在MGSRCL的基础上进一步提升少样本分类结果。

本文使用MGSRCL模型和SVMSMA模型分别对数据间的多种样本关系以及多种空间映射关系进行了建模，有效利用了数据中的多元关系，通过多粒度样本关系建模提升了视觉特征提取网络的特征提取能力，通过语义-视觉多空间关系建模提升了模型的泛化能力，从而取得了较好的少样本分类结果。

\section[\hspace{-2pt}未来展望]{{\heiti\zihao{-3} \hspace{-8pt}未来展望}}\label{section5: 未来展望}

本文分析了少样本分类面临的挑战，以数据中的多元关系为切入点，从多粒度样本关系建模与语义-视觉多空间建模两个角度入手，提出了多粒度样本关系对比学习模型和语义-视觉多空间映射适配模型来解决少样本分类问题，并取得了一定成果。但仍存在一定不足，后续可从以下几方面进一步研究：

（1）本文提出的MGSRCL方法中产生变换样本时使用的多是一些弱数据增强方法，其对特征提取网络性能提升产生的作用较为有限。目前诸如Mixup、CutMix、以及AugMix等强数据增强已被证明了能够提高模型泛化能力，但由于其会将不同图像融合形成一张新的图像，这使得图像类别不再是单一标签，无法应用于MGSRCL。因此，后续工作可以探讨如何将强数据增强方法引入所提出的方法，或者对方法进行改进以提高其适用性。

（2）本文通过使用CLIP模型的文本编码器作为语义特征提取网络，引入语义信息对视觉信息进行补充并取得了优异结果。在将类别名称输入文本编码器时，使用了CLIP原论文中提出的提示文本。但使用的提示文本是固定的，并不一定能够让模型输出对少样本分类任务来说最优的语义特征。因此后续可进一步研究其他提示文本或者将提示文本换成可学习参数，以获取最优的语义特征。

（3）本文中特征提取网络使用卷积网络，并没有使用近年来在很多视觉任务上取得良好表现的Transformer模型。这是因为Transformer模型一般需要大量的数据才能得到一个具有强大特征提取能力的预训练模型，而在少样本分类任务中，仅有tieredImageNet数据集规模较大，因此如何将Transformer模型引入少样本分类任务并取得像在其他任务上超越卷积网络的效果也是后续研究方向之一。